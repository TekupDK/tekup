# üöÄ Render Deployment Strategy - RenOS LLM\n\n\n\n## üéØ Recommended Approach\n\n\n\n### Phase 1: Start med Gemini (NU) ‚úÖ\n\n\n\n**Status:** Ready to deploy immediately\n\n
**Configuration:**
\n\n```yaml\n\n# render.yaml (existing service)\n\nservices:\n\n  - type: web\n\n    name: tekup-renos
    runtime: docker
    plan: starter # $7/month\n\n    envVars:\n\n      - key: LLM_PROVIDER\n\n        value: gemini
      - key: GEMINI_KEY\n\n        fromEnv: GEMINI_KEY # Set in Render dashboard\n\n      - key: GEMINI_MODEL\n\n        value: gemini-2.0-flash-exp\n\n```\n\n
**Deployment Steps:**
\n\n```bash\n\n# 1. Commit changes\n\ngit add .\n\ngit commit -m "‚ú® Add Gemini LLM integration to Friday AI"
\n\n# 2. Push to GitHub\n\ngit push origin main\n\n\n\n# 3. Render auto-deploys (webhook triggered)\n\n\n\n# 4. Add Gemini key in Render Dashboard:\n\n# Settings ‚Üí Environment ‚Üí Add Environment Variable\n\n# Name: GEMINI_KEY\n\n# Value: AIzaSyCIrKq05UNN62NTcaTBWRgN2yj1YvHwu6I\n\n```\n\n
**Expected Cost:**
\n\n- Render Starter: $7/month\n\n- Gemini API: ~$8-15/month\n\n- **Total: $15-22/month**\n\n
---
\n\n### Phase 2: Upgrade til Ollama (Senere) üí∞\n\n\n\n**When:** N√•r I n√•r 500+ chats/dag og vil spare costs\n\n
**Architecture:** Multi-service setup p√• Render\n\n\n\n```yaml\n\n# render.yaml (updated)\n\nservices:\n\n  # Backend service (existing)\n\n  - type: web\n\n    name: tekup-renos-backend\n\n    runtime: docker
    plan: standard # $25/month (for backend)\n\n    dockerfilePath: ./Dockerfile\n\n    envVars:
      - key: LLM_PROVIDER\n\n        value: ollama
      - key: OLLAMA_BASE_URL\n\n        value: http://tekup-renos-ollama:11434 # Internal URL\n\n      - key: OLLAMA_MODEL\n\n        value: llama3.1:8b\n\n      - key: GEMINI_KEY\n\n        fromEnv: GEMINI_KEY # Fallback\n\n  \n\n  # Ollama service (new)\n\n  - type: pserv # Private service (not publicly accessible)\n\n    name: tekup-renos-ollama\n\n    runtime: docker\n\n    plan: standard # $25/month (needs 8GB RAM for Llama 3.1)\n\n    dockerfilePath: ./Dockerfile.ollama\n\n    dockerContext: .
    envVars:
      - key: OLLAMA_HOST\n\n        value: 0.0.0.0\n\n```

---
\n\n## üì¶ Ollama Dockerfile Setup\n\n\n\n### Create `Dockerfile.ollama`\n\n\n\n```dockerfile\n\nFROM ollama/ollama:latest
\n\n# Pre-download models during build (faster startup)\n\n# Note: This runs during build, not runtime\n\nRUN ollama serve & \\n\n    sleep 5 && \
    ollama pull llama3.1:8b && \
    ollama pull mistral:7b && \
    killall ollama
\n\n# Expose Ollama API port\n\nEXPOSE 11434\n\n\n\n# Health check\n\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s \\n\n    CMD curl -f http://localhost:11434/api/tags || exit 1
\n\n# Start Ollama server\n\nCMD ["ollama", "serve"]\n\n```\n\n
**Build time:** ~10-15 min (downloads 4.7GB model)\n\n
---
\n\n## üîÑ Hybrid Strategy (Best of Both) üéØ\n\n\n\n**Use Ollama as primary, Gemini as fallback:**
\n\n```typescript
// src/controllers/chatController.ts (already implemented!)
const fridayAI = (() => {
    const llmProvider = appConfig.llm.LLM_PROVIDER;

    // Try Ollama first (cheap, fast)
    if (llmProvider === "ollama") {
        try {
            const { OllamaProvider } = require("../llm/ollamaProvider");
            const provider = new OllamaProvider(appConfig.llm.OLLAMA_BASE_URL);
            
            logger.info("Using Ollama provider");
            return new FridayAI(provider);
        } catch (error) {
            logger.warn("Ollama unavailable, falling back to Gemini");
        }
    }

    // Fallback to Gemini
    if (appConfig.llm.GEMINI_KEY) {
        const provider = new GeminiProvider(appConfig.llm.GEMINI_KEY);
        logger.info("Using Gemini fallback provider");
        return new FridayAI(provider);
    }

    // Ultimate fallback: heuristic
    return new FridayAI();
})();\n\n```

**Environment Variables:**
\n\n```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://tekup-renos-ollama:11434
OLLAMA_MODEL=llama3.1:8b
GEMINI_KEY=AIzaSy... # Fallback hvis Ollama fejler\n\n```\n\n
---
\n\n## üí∞ Cost Comparison\n\n\n\n### Current Setup (No LLM)\n\n\n\n```\n\nRender Starter: $7/month
Database (Neon): $0 (free tier)
Total: $7/month\n\n```
\n\n### Phase 1: Gemini (Recommended NOW)\n\n\n\n```\n\nRender Starter: $7/month
Gemini API: ~$10/month (1000 chats)
Database: $0
Total: $17/month\n\n```
\n\n### Phase 2: Ollama Only\n\n\n\n```\n\nRender Standard (Backend): $25/month
Render Standard (Ollama): $25/month
Database: $0
Total: $50/month

But: $0 LLM API costs!
Break-even at: ~400 chats/m√•ned vs Gemini\n\n```
\n\n### Phase 2: Hybrid (Ollama + Gemini fallback)\n\n\n\n```\n\nRender Standard x2: $50/month
Gemini API (fallback only): ~$2/month (100 fallback chats)
Total: $52/month

Savings vs pure Gemini at 1000 chats/dag:
$300/month (Gemini) ‚Üí $52/month (Hybrid) = $248/month saved! üí∞\n\n```

---
\n\n## üöÄ Deployment Timeline\n\n\n\n### Today: Deploy med Gemini ‚úÖ\n\n\n\n```bash\n\n# 1. Update .env.example\n\nLLM_PROVIDER=gemini\n\n\n\n# 2. Commit and push\n\ngit add .\n\ngit commit -m "‚ú® Add Gemini LLM + Ollama support"\n\ngit push origin main
\n\n# 3. In Render Dashboard:\n\n# Environment ‚Üí Add Variable:\n\n#   LLM_PROVIDER=gemini\n\n#   GEMINI_KEY=AIzaSyCIrKq05UNN62NTcaTBWRgN2yj1YvHwu6I\n\n\n\n# 4. Deploy (auto-triggered by push)\n\n\n\n# 5. Test:\n\ncurl -X POST https://tekup-renos.onrender.com/api/chat \\n\n  -H "Content-Type: application/json" \
  -d '{"message":"Hej Friday!"}'\n\n```

**Expected:** Friday AI responds med Gemini-powered intelligent answers!\n\n
---
\n\n### Later: Upgrade til Ollama (when 500+ chats/dag)\n\n\n\n**Prerequisites:**
\n\n- ‚úÖ Ollama tested locally\n\n- ‚úÖ Quality validated\n\n- ‚úÖ Cost analysis shows ROI\n\n- ‚úÖ Budget for $50/month hosting\n\n
**Steps:**
\n\n```bash\n\n# 1. Create Dockerfile.ollama (see above)\n\n\n\n# 2. Update render.yaml\n\n# Add Ollama private service\n\n\n\n# 3. Update environment variables\n\nLLM_PROVIDER=ollama\n\nOLLAMA_BASE_URL=http://tekup-renos-ollama:11434
\n\n# 4. Push to GitHub\n\ngit push origin main\n\n\n\n# 5. Render builds both services\n\n# Backend: ~5 min\n\n# Ollama: ~15 min (downloads model)\n\n\n\n# 6. Test hybrid fallback\n\n# Stop Ollama service ‚Üí should fallback to Gemini\n\n```\n\n
---
\n\n## üß™ Testing Strategy\n\n\n\n### 1. Local Development\n\n\n\n```bash\n\n# Test with Gemini (current)\n\nLLM_PROVIDER=gemini\n\nnpm run dev
\n\n# Test with Ollama (future)\n\nollama serve\n\nLLM_PROVIDER=ollama
npm run dev
\n\n# Test all providers\n\nnpm run llm:test\n\n```\n\n\n\n### 2. Staging (Render)\n\n\n\n```bash\n\n# Create staging branch\n\ngit checkout -b staging\n\n\n\n# Deploy to Render staging service\n\n# Test with real users (10-50 chats)\n\n# Validate response quality\n\n# Monitor costs\n\n```\n\n\n\n### 3. Production\n\n\n\n```bash\n\n# Merge to main\n\ngit checkout main\n\ngit merge staging
git push origin main
\n\n# Render auto-deploys\n\n# Monitor logs: render logs -f tekup-renos\n\n# Track errors in Render dashboard\n\n```\n\n
---
\n\n## üìä Monitoring\n\n\n\n### Key Metrics to Track\n\n\n\n```typescript\n\n// Add to chatController.ts
logger.info({
    provider: "gemini",
    responseTime: duration,
    inputTokens: estimated,
    outputTokens: estimated,
    cost: estimatedCost,
    userId: sessionId,
});\n\n```

**Dashboard queries:**
\n\n```bash\n\n# Average response time\n\ngrep "LLM response" logs | jq '.responseTime' | average\n\n\n\n# Daily cost\n\ngrep "cost" logs | jq '.cost' | sum\n\n\n\n# Error rate\n\ngrep "ERROR" logs | wc -l\n\n```\n\n
---
\n\n## ‚ö†Ô∏è Important Render Limitations\n\n\n\n### Ollama Requirements\n\n\n\n- ‚ùå **Starter plan ($7/month):** Only 512 MB RAM - TOO SMALL\n\n- ‚ö†Ô∏è **Standard plan ($25/month):** 8 GB RAM - MINIMUM for Llama 3.1 8B\n\n- ‚úÖ **Pro plan ($85/month):** 16 GB RAM - RECOMMENDED for Llama 3.1 70B\n\n\n\n### Network\n\n\n\n- ‚úÖ Private services can communicate via internal URLs\n\n- ‚úÖ `http://service-name:port` format works\n\n- ‚ùå External access to private services blocked (security)\n\n\n\n### Docker Build\n\n\n\n- ‚ö†Ô∏è 15 min timeout for builds (Ollama model download takes ~10 min)\n\n- ‚ö†Ô∏è 10 GB disk space limit (Llama 3.1 8B = 4.7 GB)\n\n- ‚úÖ Pre-download models in Dockerfile to avoid startup delays\n\n
---
\n\n## üéØ Final Recommendation\n\n\n\n### For Rendetalje.dk RIGHT NOW\n\n\n\n**‚úÖ Phase 1: Deploy med Gemini**
\n\n- Cost: $17/month\n\n- Setup: 5 minutes\n\n- Quality: ‚≠ê‚≠ê‚≠ê‚≠ê\n\n- Risk: Low (proven working)\n\n
**‚è≥ Phase 2: Evaluate Ollama efter 1 m√•ned**
\n\n- Wait until: 500+ chats/dag\n\n- Decision criteria: Cost > $50/month p√• Gemini\n\n- ROI: $200+/month savings\n\n
---
\n\n## üìù Action Items\n\n\n\n### Immediate (Today)\n\n\n\n```bash\n\n# 1. Commit changes\n\ngit add .\n\ngit commit -m "‚ú® Add Gemini LLM integration + Ollama support"\n\n\n\n# 2. Push to GitHub\n\ngit push origin main\n\n\n\n# 3. Update Render environment variables\n\n# LLM_PROVIDER=gemini\n\n# GEMINI_KEY=<your_key>\n\n\n\n# 4. Deploy (automatic)\n\n\n\n# 5. Test\n\ncurl https://tekup-renos.onrender.com/api/chat -d '{"message":"Hej!"}'\n\n```\n\n\n\n### Future (1-3 m√•neder)\n\n\n\n- [ ] Monitor Gemini costs daily\n\n- [ ] Track chat volume growth\n\n- [ ] Install Ollama lokalt for testing\n\n- [ ] Compare Ollama vs Gemini quality\n\n- [ ] When costs > $50/month ‚Üí Upgrade to Ollama\n\n- [ ] Create `Dockerfile.ollama`\n\n- [ ] Update `render.yaml` med private service\n\n- [ ] Deploy hybrid setup\n\n
---

**Status:** ‚úÖ Ready to deploy med Gemini NU!\n\n
**Next Command:** `git add . && git commit -m "‚ú® Add LLM integration"` ?
